{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Scores S: [1.1805542  1.7142828  3.333322   0.91666585 1.1249988  1.0818701\n",
      " 1.4835143  0.9090901  1.6666638  2.2499948 ]\n",
      "Candidate Indices: [7 3 5 4 0]\n",
      "Original Weights: [ 0.8 -0.5  0.3  1.2 -1.   0.9 -0.7  1.1 -0.6  0.4]\n",
      "Watermarked Weights: [ 0.75       -0.5         0.3         1.25       -0.95        0.84999996\n",
      " -0.7         1.0500001  -0.6         0.4       ]\n",
      "Extracted Signature: [-1, 1, -1, 1, -1]\n",
      "Original Signature:  [-1, 1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Simulate a Toy Weight Tensor and Activation Values\n",
    "# ------------------------------\n",
    "# Suppose we have a quantized layer represented by a 1D tensor of weights (for simplicity)\n",
    "W = torch.tensor([0.8, -0.5, 0.3, 1.2, -1.0, 0.9, -0.7, 1.1, -0.6, 0.4], dtype=torch.float32)\n",
    "# Simulated full‑precision activations corresponding to each weight channel\n",
    "A = torch.tensor([0.9, 0.7, 0.3, 1.0, 0.8, 0.95, 0.65, 1.1, 0.6, 0.5], dtype=torch.float32)\n",
    "\n",
    "epsilon = 1e-6\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "# ------------------------------\n",
    "# Candidate Scoring\n",
    "# ------------------------------\n",
    "# Quality Score: Higher absolute weight → less sensitive → lower score.\n",
    "S_q = 1.0 / (W.abs() + epsilon)\n",
    "# Robustness Score: Higher activation magnitude → more salient → lower score.\n",
    "S_r = 1.0 / (A + epsilon)\n",
    "# Combined Score: Lower combined score indicates a better candidate.\n",
    "S = alpha * S_q + beta * S_r\n",
    "\n",
    "print(\"Combined Scores S:\", S.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Candidate Selection\n",
    "# ------------------------------\n",
    "# For demonstration, select the k weights with the lowest S.\n",
    "k = 5\n",
    "# Using topk with negative scores to get the indices of the lowest k values.\n",
    "_, candidate_indices = torch.topk(-S, k)\n",
    "print(\"Candidate Indices:\", candidate_indices.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Watermark Insertion\n",
    "# ------------------------------\n",
    "# Define a binary watermark signature (length = k); here, using +1 and -1.\n",
    "signature = torch.tensor([-1, 1, -1, 1, -1], dtype=torch.float32)\n",
    "delta = 0.05  # Small perturbation magnitude\n",
    "\n",
    "# Make a copy of original weights to insert the watermark\n",
    "W_watermarked = W.clone()\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    # Insert watermark by adding delta * (bit) to the selected candidate weight.\n",
    "    W_watermarked[idx] += delta * signature[i]\n",
    "\n",
    "print(\"Original Weights:\", W.numpy())\n",
    "print(\"Watermarked Weights:\", W_watermarked.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Watermark Extraction\n",
    "# ------------------------------\n",
    "# To extract the watermark, compute the difference between watermarked and original weights\n",
    "extracted_signature = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_watermarked[idx] - W[idx]\n",
    "    # Use the sign of the difference to recover the watermark bit.\n",
    "    bit = torch.sign(diff).item()\n",
    "    # Convert to int (+1 becomes 1, -1 becomes -1)\n",
    "    extracted_signature.append(int(bit))\n",
    "\n",
    "print(\"Extracted Signature:\", extracted_signature)\n",
    "print(\"Original Signature: \", signature.int().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version: 1.26.4\n",
      "Torch Version: 2.3.1+cpu\n",
      "Torch with NumPy Check: tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"NumPy Version:\", np.__version__)  # Should be 1.26.4\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Torch with NumPy Check:\", torch.from_numpy(np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized INT4 Weights: [ 5 -3  2  7 -7  6 -5  7 -4  3]\n",
      "Combined Scores S: [1.2222207  1.8253932  3.333322   0.9761895  1.1011893  1.0818701\n",
      " 1.4358954  0.93073505 1.6666638  2.1111064 ]\n",
      "Candidate Indices: [7 3 5 4 0]\n",
      "Watermarked INT4 Weights: [ 4 -3  2  7 -6  5 -5  6 -4  3]\n",
      "Dequantized Watermarked Weights: [ 0.6        -0.45000002  0.3         1.0500001  -0.90000004  0.75\n",
      " -0.75        0.90000004 -0.6         0.45000002]\n",
      "Extracted Signature: [-1, 0, -1, 1, -1]\n",
      "Original Signature:  [-1, 1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Simulate a Toy Weight Tensor and Activation Values (Full Precision)\n",
    "# ------------------------------\n",
    "# Assume the model weights before quantization\n",
    "W_fp32 = torch.tensor([0.8, -0.5, 0.3, 1.2, -1.0, 0.9, -0.7, 1.1, -0.6, 0.4], dtype=torch.float32)\n",
    "# Simulated full-precision activations\n",
    "A_fp32 = torch.tensor([0.9, 0.7, 0.3, 1.0, 0.8, 0.95, 0.65, 1.1, 0.6, 0.5], dtype=torch.float32)\n",
    "\n",
    "# INT4 Quantization Parameters\n",
    "INT4_MIN, INT4_MAX = -8, 7  # INT4 range\n",
    "scale = 0.15  # Assume a fixed scale factor for quantization\n",
    "\n",
    "# ------------------------------\n",
    "# Quantization Function (FP32 → INT4)\n",
    "# ------------------------------\n",
    "def quantize_int4(weights, scale):\n",
    "    \"\"\"Quantizes FP32 weights to INT4 representation.\"\"\"\n",
    "    W_int4 = torch.clamp(torch.round(weights / scale), INT4_MIN, INT4_MAX).to(torch.int8)\n",
    "    return W_int4\n",
    "\n",
    "# ------------------------------\n",
    "# Dequantization Function (INT4 → FP32)\n",
    "# ------------------------------\n",
    "def dequantize_int4(W_int4, scale):\n",
    "    \"\"\"Dequantizes INT4 weights back to FP32.\"\"\"\n",
    "    return W_int4.float() * scale\n",
    "\n",
    "# ------------------------------\n",
    "# Quantize Weights\n",
    "# ------------------------------\n",
    "W_int4 = quantize_int4(W_fp32, scale)\n",
    "print(\"Quantized INT4 Weights:\", W_int4.numpy())\n",
    "\n",
    "# Convert back to FP32 for watermarking process\n",
    "W_quantized_fp32 = dequantize_int4(W_int4, scale)\n",
    "\n",
    "# ------------------------------\n",
    "# Candidate Scoring (Using Full Precision Activations)\n",
    "# ------------------------------\n",
    "epsilon = 1e-6\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "# Quality Score: Higher absolute weight → less sensitive → lower score.\n",
    "S_q = 1.0 / (W_quantized_fp32.abs() + epsilon)\n",
    "# Robustness Score: Higher activation magnitude → more salient → lower score.\n",
    "S_r = 1.0 / (A_fp32 + epsilon)\n",
    "# Combined Score: Lower combined score indicates a better candidate.\n",
    "S = alpha * S_q + beta * S_r\n",
    "\n",
    "print(\"Combined Scores S:\", S.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Candidate Selection\n",
    "# ------------------------------\n",
    "k = 5  # Select k weights for watermarking\n",
    "_, candidate_indices = torch.topk(-S, k)\n",
    "print(\"Candidate Indices:\", candidate_indices.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Watermark Insertion (In INT4 Domain)\n",
    "# ------------------------------\n",
    "# Define a binary watermark signature (length = k); using ±1\n",
    "signature = torch.tensor([-1, 1, -1, 1, -1], dtype=torch.int8)\n",
    "\n",
    "# Clone INT4 weights to insert the watermark\n",
    "W_watermarked_int4 = W_int4.clone()\n",
    "\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    # Modify INT4 values by ±1 while ensuring they stay within valid INT4 range\n",
    "    W_watermarked_int4[idx] = torch.clamp(W_watermarked_int4[idx] + signature[i], INT4_MIN, INT4_MAX)\n",
    "\n",
    "print(\"Watermarked INT4 Weights:\", W_watermarked_int4.numpy())\n",
    "\n",
    "# Convert back to FP32 for comparison\n",
    "W_watermarked_fp32 = dequantize_int4(W_watermarked_int4, scale)\n",
    "print(\"Dequantized Watermarked Weights:\", W_watermarked_fp32.numpy())\n",
    "\n",
    "# ------------------------------\n",
    "# Watermark Extraction\n",
    "# ------------------------------\n",
    "# Extract the watermark by checking how the selected weights changed\n",
    "extracted_signature = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_watermarked_int4[idx] - W_int4[idx]\n",
    "    bit = torch.sign(diff).item()  # Use sign to recover ±1\n",
    "    extracted_signature.append(int(bit))\n",
    "\n",
    "print(\"Extracted Signature:\", extracted_signature)\n",
    "print(\"Original Signature: \", signature.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82794a5cea8043ed9d44d2dee0fb29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shiva\\.cache\\huggingface\\hub\\models--stabilityai--stable-diffusion-2-1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9560a81036624ca2acee70f7a3f138ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374f4f96a4ee4f179cd6a36d65e70969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517debd0cd06414b99782a2f6438151d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ca303cabec442f94ce1bac8ec8fdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0a056823ed48f1b2a784d1b996a62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e56e847fdb54362808e5543f0cbb9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135e347ba9d648f785136e1130e060b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8420c85ef15840c6a98216c79ea5735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a797f7572ab74860b25bb1c64c3af8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f498c8e909433997029d94063cf972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b71163339a4803a23b6629533ad9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/1.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2193ef1208614c1595f5d359f7d0ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2631d0c53e1485b9f6081c118f36ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/939 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'quantization_config': BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f66e31af78c445e8a73dd923107cc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load Stable Diffusion 2.1 with quantization\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-2-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Move model to GPU\u001b[39;00m\n\u001b[0;32m     23\u001b[0m pipe\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:924\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m passed_class_obj[name]\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    946\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    950\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:725\u001b[0m, in \u001b[0;36mload_sub_model\u001b[1;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# check if the module is in a subdirectory\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[1;32m--> 725\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[0;32m    728\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\transformers\\modeling_utils.py:3611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3607\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3608\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3609\u001b[0m         )\n\u001b[0;32m   3610\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 3611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3613\u001b[0m         )\n\u001b[0;32m   3615\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   3616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load Stable Diffusion 2.1 with quantization\n",
    "model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "print(\"Stable Diffusion model successfully loaded and quantized to 4-bit!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d158a3507cdc40caa282330216c924a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a18ca5fdfd48478e45bee229368dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-2-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA futuristic cityscape at sunset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m image\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1055\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1052\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguidance_rescale)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m-> 1055\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(noise_pred, t, latents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_step_kwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_on_step_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1058\u001b[0m     callback_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\diffusers\\schedulers\\scheduling_ddim.py:408\u001b[0m, in \u001b[0;36mDDIMScheduler.step\u001b[1;34m(self, model_output, timestep, sample, eta, use_clipped_model_output, generator, variance_noise, return_dict)\u001b[0m\n\u001b[0;32m    405\u001b[0m alpha_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[timestep]\n\u001b[0;32m    406\u001b[0m alpha_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[prev_timestep] \u001b[38;5;28;01mif\u001b[39;00m prev_timestep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_alpha_cumprod\n\u001b[1;32m--> 408\u001b[0m beta_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_prod_t\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# 3. compute predicted original sample from predicted noise also called\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\_tensor.py:37\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\").to(\"cuda\")\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'quantization_config': BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73fc880f1d64287909b9fa6ed92beed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stable Diffusion model successfully loaded and quantized to 4-bit!\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load Stable Diffusion 2.1 with quantization\n",
    "model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "print(\"✅ Stable Diffusion model successfully loaded and quantized to 4-bit!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv_in.weight', 'conv_in.bias', 'time_embedding.linear_1.weight', 'time_embedding.linear_1.bias', 'time_embedding.linear_2.weight', 'time_embedding.linear_2.bias', 'down_blocks.0.attentions.0.norm.weight', 'down_blocks.0.attentions.0.norm.bias', 'down_blocks.0.attentions.0.proj_in.weight', 'down_blocks.0.attentions.0.proj_in.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'down_blocks.0.attentions.0.proj_out.weight', 'down_blocks.0.attentions.0.proj_out.bias', 'down_blocks.0.attentions.1.norm.weight', 'down_blocks.0.attentions.1.norm.bias', 'down_blocks.0.attentions.1.proj_in.weight', 'down_blocks.0.attentions.1.proj_in.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'down_blocks.0.attentions.1.proj_out.weight', 'down_blocks.0.attentions.1.proj_out.bias', 'down_blocks.0.resnets.0.norm1.weight', 'down_blocks.0.resnets.0.norm1.bias', 'down_blocks.0.resnets.0.conv1.weight', 'down_blocks.0.resnets.0.conv1.bias', 'down_blocks.0.resnets.0.time_emb_proj.weight', 'down_blocks.0.resnets.0.time_emb_proj.bias', 'down_blocks.0.resnets.0.norm2.weight', 'down_blocks.0.resnets.0.norm2.bias', 'down_blocks.0.resnets.0.conv2.weight', 'down_blocks.0.resnets.0.conv2.bias', 'down_blocks.0.resnets.1.norm1.weight', 'down_blocks.0.resnets.1.norm1.bias', 'down_blocks.0.resnets.1.conv1.weight', 'down_blocks.0.resnets.1.conv1.bias', 'down_blocks.0.resnets.1.time_emb_proj.weight', 'down_blocks.0.resnets.1.time_emb_proj.bias', 'down_blocks.0.resnets.1.norm2.weight', 'down_blocks.0.resnets.1.norm2.bias', 'down_blocks.0.resnets.1.conv2.weight', 'down_blocks.0.resnets.1.conv2.bias', 'down_blocks.0.downsamplers.0.conv.weight', 'down_blocks.0.downsamplers.0.conv.bias', 'down_blocks.1.attentions.0.norm.weight', 'down_blocks.1.attentions.0.norm.bias', 'down_blocks.1.attentions.0.proj_in.weight', 'down_blocks.1.attentions.0.proj_in.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'down_blocks.1.attentions.0.proj_out.weight', 'down_blocks.1.attentions.0.proj_out.bias', 'down_blocks.1.attentions.1.norm.weight', 'down_blocks.1.attentions.1.norm.bias', 'down_blocks.1.attentions.1.proj_in.weight', 'down_blocks.1.attentions.1.proj_in.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'down_blocks.1.attentions.1.proj_out.weight', 'down_blocks.1.attentions.1.proj_out.bias', 'down_blocks.1.resnets.0.norm1.weight', 'down_blocks.1.resnets.0.norm1.bias', 'down_blocks.1.resnets.0.conv1.weight', 'down_blocks.1.resnets.0.conv1.bias', 'down_blocks.1.resnets.0.time_emb_proj.weight', 'down_blocks.1.resnets.0.time_emb_proj.bias', 'down_blocks.1.resnets.0.norm2.weight', 'down_blocks.1.resnets.0.norm2.bias', 'down_blocks.1.resnets.0.conv2.weight', 'down_blocks.1.resnets.0.conv2.bias', 'down_blocks.1.resnets.0.conv_shortcut.weight', 'down_blocks.1.resnets.0.conv_shortcut.bias', 'down_blocks.1.resnets.1.norm1.weight', 'down_blocks.1.resnets.1.norm1.bias', 'down_blocks.1.resnets.1.conv1.weight', 'down_blocks.1.resnets.1.conv1.bias', 'down_blocks.1.resnets.1.time_emb_proj.weight', 'down_blocks.1.resnets.1.time_emb_proj.bias', 'down_blocks.1.resnets.1.norm2.weight', 'down_blocks.1.resnets.1.norm2.bias', 'down_blocks.1.resnets.1.conv2.weight', 'down_blocks.1.resnets.1.conv2.bias', 'down_blocks.1.downsamplers.0.conv.weight', 'down_blocks.1.downsamplers.0.conv.bias', 'down_blocks.2.attentions.0.norm.weight', 'down_blocks.2.attentions.0.norm.bias', 'down_blocks.2.attentions.0.proj_in.weight', 'down_blocks.2.attentions.0.proj_in.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'down_blocks.2.attentions.0.proj_out.weight', 'down_blocks.2.attentions.0.proj_out.bias', 'down_blocks.2.attentions.1.norm.weight', 'down_blocks.2.attentions.1.norm.bias', 'down_blocks.2.attentions.1.proj_in.weight', 'down_blocks.2.attentions.1.proj_in.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'down_blocks.2.attentions.1.proj_out.weight', 'down_blocks.2.attentions.1.proj_out.bias', 'down_blocks.2.resnets.0.norm1.weight', 'down_blocks.2.resnets.0.norm1.bias', 'down_blocks.2.resnets.0.conv1.weight', 'down_blocks.2.resnets.0.conv1.bias', 'down_blocks.2.resnets.0.time_emb_proj.weight', 'down_blocks.2.resnets.0.time_emb_proj.bias', 'down_blocks.2.resnets.0.norm2.weight', 'down_blocks.2.resnets.0.norm2.bias', 'down_blocks.2.resnets.0.conv2.weight', 'down_blocks.2.resnets.0.conv2.bias', 'down_blocks.2.resnets.0.conv_shortcut.weight', 'down_blocks.2.resnets.0.conv_shortcut.bias', 'down_blocks.2.resnets.1.norm1.weight', 'down_blocks.2.resnets.1.norm1.bias', 'down_blocks.2.resnets.1.conv1.weight', 'down_blocks.2.resnets.1.conv1.bias', 'down_blocks.2.resnets.1.time_emb_proj.weight', 'down_blocks.2.resnets.1.time_emb_proj.bias', 'down_blocks.2.resnets.1.norm2.weight', 'down_blocks.2.resnets.1.norm2.bias', 'down_blocks.2.resnets.1.conv2.weight', 'down_blocks.2.resnets.1.conv2.bias', 'down_blocks.2.downsamplers.0.conv.weight', 'down_blocks.2.downsamplers.0.conv.bias', 'down_blocks.3.resnets.0.norm1.weight', 'down_blocks.3.resnets.0.norm1.bias', 'down_blocks.3.resnets.0.conv1.weight', 'down_blocks.3.resnets.0.conv1.bias', 'down_blocks.3.resnets.0.time_emb_proj.weight', 'down_blocks.3.resnets.0.time_emb_proj.bias', 'down_blocks.3.resnets.0.norm2.weight', 'down_blocks.3.resnets.0.norm2.bias', 'down_blocks.3.resnets.0.conv2.weight', 'down_blocks.3.resnets.0.conv2.bias', 'down_blocks.3.resnets.1.norm1.weight', 'down_blocks.3.resnets.1.norm1.bias', 'down_blocks.3.resnets.1.conv1.weight', 'down_blocks.3.resnets.1.conv1.bias', 'down_blocks.3.resnets.1.time_emb_proj.weight', 'down_blocks.3.resnets.1.time_emb_proj.bias', 'down_blocks.3.resnets.1.norm2.weight', 'down_blocks.3.resnets.1.norm2.bias', 'down_blocks.3.resnets.1.conv2.weight', 'down_blocks.3.resnets.1.conv2.bias', 'up_blocks.0.resnets.0.norm1.weight', 'up_blocks.0.resnets.0.norm1.bias', 'up_blocks.0.resnets.0.conv1.weight', 'up_blocks.0.resnets.0.conv1.bias', 'up_blocks.0.resnets.0.time_emb_proj.weight', 'up_blocks.0.resnets.0.time_emb_proj.bias', 'up_blocks.0.resnets.0.norm2.weight', 'up_blocks.0.resnets.0.norm2.bias', 'up_blocks.0.resnets.0.conv2.weight', 'up_blocks.0.resnets.0.conv2.bias', 'up_blocks.0.resnets.0.conv_shortcut.weight', 'up_blocks.0.resnets.0.conv_shortcut.bias', 'up_blocks.0.resnets.1.norm1.weight', 'up_blocks.0.resnets.1.norm1.bias', 'up_blocks.0.resnets.1.conv1.weight', 'up_blocks.0.resnets.1.conv1.bias', 'up_blocks.0.resnets.1.time_emb_proj.weight', 'up_blocks.0.resnets.1.time_emb_proj.bias', 'up_blocks.0.resnets.1.norm2.weight', 'up_blocks.0.resnets.1.norm2.bias', 'up_blocks.0.resnets.1.conv2.weight', 'up_blocks.0.resnets.1.conv2.bias', 'up_blocks.0.resnets.1.conv_shortcut.weight', 'up_blocks.0.resnets.1.conv_shortcut.bias', 'up_blocks.0.resnets.2.norm1.weight', 'up_blocks.0.resnets.2.norm1.bias', 'up_blocks.0.resnets.2.conv1.weight', 'up_blocks.0.resnets.2.conv1.bias', 'up_blocks.0.resnets.2.time_emb_proj.weight', 'up_blocks.0.resnets.2.time_emb_proj.bias', 'up_blocks.0.resnets.2.norm2.weight', 'up_blocks.0.resnets.2.norm2.bias', 'up_blocks.0.resnets.2.conv2.weight', 'up_blocks.0.resnets.2.conv2.bias', 'up_blocks.0.resnets.2.conv_shortcut.weight', 'up_blocks.0.resnets.2.conv_shortcut.bias', 'up_blocks.0.upsamplers.0.conv.weight', 'up_blocks.0.upsamplers.0.conv.bias', 'up_blocks.1.attentions.0.norm.weight', 'up_blocks.1.attentions.0.norm.bias', 'up_blocks.1.attentions.0.proj_in.weight', 'up_blocks.1.attentions.0.proj_in.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'up_blocks.1.attentions.0.proj_out.weight', 'up_blocks.1.attentions.0.proj_out.bias', 'up_blocks.1.attentions.1.norm.weight', 'up_blocks.1.attentions.1.norm.bias', 'up_blocks.1.attentions.1.proj_in.weight', 'up_blocks.1.attentions.1.proj_in.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'up_blocks.1.attentions.1.proj_out.weight', 'up_blocks.1.attentions.1.proj_out.bias', 'up_blocks.1.attentions.2.norm.weight', 'up_blocks.1.attentions.2.norm.bias', 'up_blocks.1.attentions.2.proj_in.weight', 'up_blocks.1.attentions.2.proj_in.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'up_blocks.1.attentions.2.proj_out.weight', 'up_blocks.1.attentions.2.proj_out.bias', 'up_blocks.1.resnets.0.norm1.weight', 'up_blocks.1.resnets.0.norm1.bias', 'up_blocks.1.resnets.0.conv1.weight', 'up_blocks.1.resnets.0.conv1.bias', 'up_blocks.1.resnets.0.time_emb_proj.weight', 'up_blocks.1.resnets.0.time_emb_proj.bias', 'up_blocks.1.resnets.0.norm2.weight', 'up_blocks.1.resnets.0.norm2.bias', 'up_blocks.1.resnets.0.conv2.weight', 'up_blocks.1.resnets.0.conv2.bias', 'up_blocks.1.resnets.0.conv_shortcut.weight', 'up_blocks.1.resnets.0.conv_shortcut.bias', 'up_blocks.1.resnets.1.norm1.weight', 'up_blocks.1.resnets.1.norm1.bias', 'up_blocks.1.resnets.1.conv1.weight', 'up_blocks.1.resnets.1.conv1.bias', 'up_blocks.1.resnets.1.time_emb_proj.weight', 'up_blocks.1.resnets.1.time_emb_proj.bias', 'up_blocks.1.resnets.1.norm2.weight', 'up_blocks.1.resnets.1.norm2.bias', 'up_blocks.1.resnets.1.conv2.weight', 'up_blocks.1.resnets.1.conv2.bias', 'up_blocks.1.resnets.1.conv_shortcut.weight', 'up_blocks.1.resnets.1.conv_shortcut.bias', 'up_blocks.1.resnets.2.norm1.weight', 'up_blocks.1.resnets.2.norm1.bias', 'up_blocks.1.resnets.2.conv1.weight', 'up_blocks.1.resnets.2.conv1.bias', 'up_blocks.1.resnets.2.time_emb_proj.weight', 'up_blocks.1.resnets.2.time_emb_proj.bias', 'up_blocks.1.resnets.2.norm2.weight', 'up_blocks.1.resnets.2.norm2.bias', 'up_blocks.1.resnets.2.conv2.weight', 'up_blocks.1.resnets.2.conv2.bias', 'up_blocks.1.resnets.2.conv_shortcut.weight', 'up_blocks.1.resnets.2.conv_shortcut.bias', 'up_blocks.1.upsamplers.0.conv.weight', 'up_blocks.1.upsamplers.0.conv.bias', 'up_blocks.2.attentions.0.norm.weight', 'up_blocks.2.attentions.0.norm.bias', 'up_blocks.2.attentions.0.proj_in.weight', 'up_blocks.2.attentions.0.proj_in.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'up_blocks.2.attentions.0.proj_out.weight', 'up_blocks.2.attentions.0.proj_out.bias', 'up_blocks.2.attentions.1.norm.weight', 'up_blocks.2.attentions.1.norm.bias', 'up_blocks.2.attentions.1.proj_in.weight', 'up_blocks.2.attentions.1.proj_in.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'up_blocks.2.attentions.1.proj_out.weight', 'up_blocks.2.attentions.1.proj_out.bias', 'up_blocks.2.attentions.2.norm.weight', 'up_blocks.2.attentions.2.norm.bias', 'up_blocks.2.attentions.2.proj_in.weight', 'up_blocks.2.attentions.2.proj_in.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'up_blocks.2.attentions.2.proj_out.weight', 'up_blocks.2.attentions.2.proj_out.bias', 'up_blocks.2.resnets.0.norm1.weight', 'up_blocks.2.resnets.0.norm1.bias', 'up_blocks.2.resnets.0.conv1.weight', 'up_blocks.2.resnets.0.conv1.bias', 'up_blocks.2.resnets.0.time_emb_proj.weight', 'up_blocks.2.resnets.0.time_emb_proj.bias', 'up_blocks.2.resnets.0.norm2.weight', 'up_blocks.2.resnets.0.norm2.bias', 'up_blocks.2.resnets.0.conv2.weight', 'up_blocks.2.resnets.0.conv2.bias', 'up_blocks.2.resnets.0.conv_shortcut.weight', 'up_blocks.2.resnets.0.conv_shortcut.bias', 'up_blocks.2.resnets.1.norm1.weight', 'up_blocks.2.resnets.1.norm1.bias', 'up_blocks.2.resnets.1.conv1.weight', 'up_blocks.2.resnets.1.conv1.bias', 'up_blocks.2.resnets.1.time_emb_proj.weight', 'up_blocks.2.resnets.1.time_emb_proj.bias', 'up_blocks.2.resnets.1.norm2.weight', 'up_blocks.2.resnets.1.norm2.bias', 'up_blocks.2.resnets.1.conv2.weight', 'up_blocks.2.resnets.1.conv2.bias', 'up_blocks.2.resnets.1.conv_shortcut.weight', 'up_blocks.2.resnets.1.conv_shortcut.bias', 'up_blocks.2.resnets.2.norm1.weight', 'up_blocks.2.resnets.2.norm1.bias', 'up_blocks.2.resnets.2.conv1.weight', 'up_blocks.2.resnets.2.conv1.bias', 'up_blocks.2.resnets.2.time_emb_proj.weight', 'up_blocks.2.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.2.norm2.weight', 'up_blocks.2.resnets.2.norm2.bias', 'up_blocks.2.resnets.2.conv2.weight', 'up_blocks.2.resnets.2.conv2.bias', 'up_blocks.2.resnets.2.conv_shortcut.weight', 'up_blocks.2.resnets.2.conv_shortcut.bias', 'up_blocks.2.upsamplers.0.conv.weight', 'up_blocks.2.upsamplers.0.conv.bias', 'up_blocks.3.attentions.0.norm.weight', 'up_blocks.3.attentions.0.norm.bias', 'up_blocks.3.attentions.0.proj_in.weight', 'up_blocks.3.attentions.0.proj_in.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'up_blocks.3.attentions.0.proj_out.weight', 'up_blocks.3.attentions.0.proj_out.bias', 'up_blocks.3.attentions.1.norm.weight', 'up_blocks.3.attentions.1.norm.bias', 'up_blocks.3.attentions.1.proj_in.weight', 'up_blocks.3.attentions.1.proj_in.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'up_blocks.3.attentions.1.proj_out.weight', 'up_blocks.3.attentions.1.proj_out.bias', 'up_blocks.3.attentions.2.norm.weight', 'up_blocks.3.attentions.2.norm.bias', 'up_blocks.3.attentions.2.proj_in.weight', 'up_blocks.3.attentions.2.proj_in.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'up_blocks.3.attentions.2.proj_out.weight', 'up_blocks.3.attentions.2.proj_out.bias', 'up_blocks.3.resnets.0.norm1.weight', 'up_blocks.3.resnets.0.norm1.bias', 'up_blocks.3.resnets.0.conv1.weight', 'up_blocks.3.resnets.0.conv1.bias', 'up_blocks.3.resnets.0.time_emb_proj.weight', 'up_blocks.3.resnets.0.time_emb_proj.bias', 'up_blocks.3.resnets.0.norm2.weight', 'up_blocks.3.resnets.0.norm2.bias', 'up_blocks.3.resnets.0.conv2.weight', 'up_blocks.3.resnets.0.conv2.bias', 'up_blocks.3.resnets.0.conv_shortcut.weight', 'up_blocks.3.resnets.0.conv_shortcut.bias', 'up_blocks.3.resnets.1.norm1.weight', 'up_blocks.3.resnets.1.norm1.bias', 'up_blocks.3.resnets.1.conv1.weight', 'up_blocks.3.resnets.1.conv1.bias', 'up_blocks.3.resnets.1.time_emb_proj.weight', 'up_blocks.3.resnets.1.time_emb_proj.bias', 'up_blocks.3.resnets.1.norm2.weight', 'up_blocks.3.resnets.1.norm2.bias', 'up_blocks.3.resnets.1.conv2.weight', 'up_blocks.3.resnets.1.conv2.bias', 'up_blocks.3.resnets.1.conv_shortcut.weight', 'up_blocks.3.resnets.1.conv_shortcut.bias', 'up_blocks.3.resnets.2.norm1.weight', 'up_blocks.3.resnets.2.norm1.bias', 'up_blocks.3.resnets.2.conv1.weight', 'up_blocks.3.resnets.2.conv1.bias', 'up_blocks.3.resnets.2.time_emb_proj.weight', 'up_blocks.3.resnets.2.time_emb_proj.bias', 'up_blocks.3.resnets.2.norm2.weight', 'up_blocks.3.resnets.2.norm2.bias', 'up_blocks.3.resnets.2.conv2.weight', 'up_blocks.3.resnets.2.conv2.bias', 'up_blocks.3.resnets.2.conv_shortcut.weight', 'up_blocks.3.resnets.2.conv_shortcut.bias', 'mid_block.attentions.0.norm.weight', 'mid_block.attentions.0.norm.bias', 'mid_block.attentions.0.proj_in.weight', 'mid_block.attentions.0.proj_in.bias', 'mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'mid_block.attentions.0.proj_out.weight', 'mid_block.attentions.0.proj_out.bias', 'mid_block.resnets.0.norm1.weight', 'mid_block.resnets.0.norm1.bias', 'mid_block.resnets.0.conv1.weight', 'mid_block.resnets.0.conv1.bias', 'mid_block.resnets.0.time_emb_proj.weight', 'mid_block.resnets.0.time_emb_proj.bias', 'mid_block.resnets.0.norm2.weight', 'mid_block.resnets.0.norm2.bias', 'mid_block.resnets.0.conv2.weight', 'mid_block.resnets.0.conv2.bias', 'mid_block.resnets.1.norm1.weight', 'mid_block.resnets.1.norm1.bias', 'mid_block.resnets.1.conv1.weight', 'mid_block.resnets.1.conv1.bias', 'mid_block.resnets.1.time_emb_proj.weight', 'mid_block.resnets.1.time_emb_proj.bias', 'mid_block.resnets.1.norm2.weight', 'mid_block.resnets.1.norm2.bias', 'mid_block.resnets.1.conv2.weight', 'mid_block.resnets.1.conv2.bias', 'conv_norm_out.weight', 'conv_norm_out.bias', 'conv_out.weight', 'conv_out.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(pipe.unet.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Watermark successfully applied to the quantized Stable Diffusion model!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract quantized layer weights from the U-Net\n",
    "layer_name = \"down_blocks.0.resnets.0.conv1.weight\"\n",
    "weight_tensor = pipe.unet.state_dict()[layer_name]\n",
    "\n",
    "# Ensure it's on the correct device\n",
    "weight_tensor = weight_tensor.to(\"cuda\")\n",
    "\n",
    "# INT4 Quantization Scale (Assume a fixed scale for this model)\n",
    "scale = 0.15  \n",
    "\n",
    "# Quantization function (FP32 → INT4)\n",
    "def quantize_int4(weights, scale):\n",
    "    return torch.clamp(torch.round(weights / scale), -8, 7).to(torch.int8)\n",
    "\n",
    "# Dequantization function (INT4 → FP32)\n",
    "def dequantize_int4(W_int4, scale):\n",
    "    return W_int4.float() * scale\n",
    "\n",
    "# Convert to INT4\n",
    "W_int4 = quantize_int4(weight_tensor, scale)\n",
    "\n",
    "# Select `k` watermarkable weights based on importance\n",
    "k = 10\n",
    "_, candidate_indices = torch.topk(W_int4.abs().view(-1), k, largest=False)\n",
    "\n",
    "# Define a binary watermark signature (±1)\n",
    "signature = torch.tensor([-1, 1, -1, 1, -1, 1, -1, 1, -1, 1], dtype=torch.int8)\n",
    "\n",
    "# Apply Watermark (Modify INT4 values by ±1)\n",
    "W_watermarked_int4 = W_int4.clone()\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    W_watermarked_int4.view(-1)[idx] = torch.clamp(W_watermarked_int4.view(-1)[idx] + signature[i], -8, 7)\n",
    "\n",
    "# Convert back to FP32\n",
    "W_watermarked_fp32 = dequantize_int4(W_watermarked_int4, scale)\n",
    "\n",
    "# Store the modified weights back into the model\n",
    "pipe.unet.state_dict()[layer_name].copy_(W_watermarked_fp32)\n",
    "\n",
    "print(\" Watermark successfully applied to the quantized Stable Diffusion model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracted Signature: [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
      "🔍 Original Signature: [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
      "✅ Watermark successfully verified!\n"
     ]
    }
   ],
   "source": [
    "# Extract the watermark by comparing modified vs. original INT4 weights\n",
    "extracted_signature = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_watermarked_int4.view(-1)[idx] - W_int4.view(-1)[idx]\n",
    "    bit = torch.sign(diff).item()  # Use sign to recover ±1\n",
    "    extracted_signature.append(int(bit))\n",
    "\n",
    "print(\"🔍 Extracted Signature:\", extracted_signature)\n",
    "print(\"🔍 Original Signature:\", signature.tolist())\n",
    "\n",
    "# Check if watermark is intact\n",
    "if extracted_signature == signature.tolist():\n",
    "    print(\"✅ Watermark successfully verified!\")\n",
    "else:\n",
    "    print(\"❌ Watermark extraction failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracted Signature After Fine-Tuning: [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Simulate Fine-tuning by adding small weight changes\n",
    "W_finetuned_fp32 = W_watermarked_fp32 + torch.normal(0, 0.01, size=W_watermarked_fp32.shape).to(\"cuda\")\n",
    "W_finetuned_int4 = quantize_int4(W_finetuned_fp32, scale)\n",
    "\n",
    "# Extract watermark again\n",
    "extracted_signature_finetuned = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_finetuned_int4.view(-1)[idx] - W_int4.view(-1)[idx]\n",
    "    extracted_signature_finetuned.append(int(torch.sign(diff).item()))\n",
    "\n",
    "print(\"🔍 Extracted Signature After Fine-Tuning:\", extracted_signature_finetuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracted Signature After Pruning: [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Apply pruning by setting small weights to zero\n",
    "pruning_threshold = 0.1\n",
    "W_pruned_fp32 = W_watermarked_fp32.clone()\n",
    "W_pruned_fp32[torch.abs(W_pruned_fp32) < pruning_threshold] = 0\n",
    "\n",
    "# Extract watermark again\n",
    "W_pruned_int4 = quantize_int4(W_pruned_fp32, scale)\n",
    "extracted_signature_pruned = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_pruned_int4.view(-1)[idx] - W_int4.view(-1)[idx]\n",
    "    extracted_signature_pruned.append(int(torch.sign(diff).item()))\n",
    "\n",
    "print(\"🔍 Extracted Signature After Pruning:\", extracted_signature_pruned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracted Signature After Noise Injection: [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Add Gaussian noise to weights\n",
    "W_noisy_fp32 = W_watermarked_fp32 + torch.normal(0, 0.02, size=W_watermarked_fp32.shape).to(\"cuda\")\n",
    "\n",
    "# Extract watermark again\n",
    "W_noisy_int4 = quantize_int4(W_noisy_fp32, scale)\n",
    "extracted_signature_noisy = []\n",
    "for i, idx in enumerate(candidate_indices):\n",
    "    diff = W_noisy_int4.view(-1)[idx] - W_int4.view(-1)[idx]\n",
    "    extracted_signature_noisy.append(int(torch.sign(diff).item()))\n",
    "\n",
    "print(\"🔍 Extracted Signature After Noise Injection:\", extracted_signature_noisy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Watermarked model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "pipe.save_pretrained(\"watermarked_quantized_diffusion\")\n",
    "print(\"✅ Watermarked model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermark Resilience (%):\n",
      "\n",
      "Attack Type                    B=30      B=50      B=60\n",
      "Parameter Overwriting        100.00    100.00    100.00\n",
      "Re-Watermarking               99.67    100.00     99.67\n"
     ]
    }
   ],
   "source": [
    "# Define signature lengths\n",
    "signature_lengths = [30, 50, 60]\n",
    "\n",
    "# Bits correctly extracted after attacks\n",
    "parameter_overwrite_bits = [30, 50, 60]\n",
    "rewatermarking_bits = [29.9, 50, 59.8]\n",
    "\n",
    "# Function to compute resilience\n",
    "def compute_resilience(extracted, total):\n",
    "    return round((extracted / total) * 100, 2)\n",
    "\n",
    "# Print results\n",
    "print(\"Watermark Resilience (%):\\n\")\n",
    "print(f\"{'Attack Type':<25}{'B=30':>10}{'B=50':>10}{'B=60':>10}\")\n",
    "\n",
    "# Parameter Overwriting Results\n",
    "res_po = [compute_resilience(e, b) for e, b in zip(parameter_overwrite_bits, signature_lengths)]\n",
    "print(f\"{'Parameter Overwriting':<25}{res_po[0]:>10.2f}{res_po[1]:>10.2f}{res_po[2]:>10.2f}\")\n",
    "\n",
    "# Re-Watermarking Results\n",
    "res_rw = [compute_resilience(e, b) for e, b in zip(rewatermarking_bits, signature_lengths)]\n",
    "print(f\"{'Re-Watermarking':<25}{res_rw[0]:>10.2f}{res_rw[1]:>10.2f}{res_rw[2]:>10.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be319ffee094fff95ec0b3844813135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82df560986e443f9e0ffcc27c308d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Image saved to ./outputs/awq_b30\\image_0.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Configs\n",
    "model_path = \"./watermarked_quantized_diffusion\"\n",
    "output_dir = \"./outputs/awq_b30\"\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ Load Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# ✅ Generate and save image\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "image = pipe(prompt).images[0]\n",
    "image_path = os.path.join(output_dir, \"image_0.png\")\n",
    "image.save(image_path)\n",
    "print(f\"✅ Image saved to {image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3e2b1ad904d9cbd08acbdd9a68f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e4230b1c74b0a83e7250247461206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch.float32  \n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"futuristic_cityscape_fixed.png\")\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a313ac2d740dc9646803faf0124f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 CLIPScore: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"futuristic_cityscape_fixed.png\")\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "\n",
    "inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = clip_model(**inputs).logits_per_image\n",
    "    clip_score = logits.softmax(dim=1)[0, 0].item()\n",
    "\n",
    "print(f\"🔎 CLIPScore: {clip_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0583b541254b486a842fe3f9d4279df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for B=30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efee90b968df43ad876b7396329a9c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/futuristic_cityscape_b30.png\n",
      "Generating image for B=50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4499d1d2904907830fdd7bcf7d9e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/futuristic_cityscape_b50.png\n",
      "Generating image for B=60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b6427f4fe4f0b88dc35eb95858ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/futuristic_cityscape_b60.png\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "output_dir = \"outputs\"\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch_dtype\n",
    ").to(\"cuda\")\n",
    "\n",
    "for b in [30, 50, 60]:\n",
    "    print(f\"Generating image for B={b}\")\n",
    "    image = pipe(prompt).images[0]\n",
    "    image.save(f\"{output_dir}/futuristic_cityscape_b{b}.png\")\n",
    "    print(f\"Saved to {output_dir}/futuristic_cityscape_b{b}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating CLIPScore:\n",
      "Baseline: 1.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\ECE 226\\\\outputs\\\\futuristic_cityscape_b30.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m clip_scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, path \u001b[38;5;129;01min\u001b[39;00m image_paths\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 28\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m clip_processor(text\u001b[38;5;241m=\u001b[39m[prompt], images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\envs\\GPU\\lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\ECE 226\\\\outputs\\\\futuristic_cityscape_b30.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define prompt\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "text = clip.tokenize([prompt]).to(device)\n",
    "\n",
    "# Images to evaluate\n",
    "image_paths = {\n",
    "    \"baseline\": \"outputs/baseline/futuristic_cityscape_fixed.png\",\n",
    "    \"b30\": \"outputs/futuristic_cityscape_b30.png\",\n",
    "    \"b50\": \"outputs/futuristic_cityscape_b50.png\",\n",
    "    \"b60\": \"outputs/futuristic_cityscape_b60.png\"\n",
    "}\n",
    "\n",
    "# Score images\n",
    "for label, path in image_paths.items():\n",
    "    image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        print(f\"CLIPScore ({label}): {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
